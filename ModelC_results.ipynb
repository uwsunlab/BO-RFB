{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for math\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Plotting tools\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#File Tools for local\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#Sampling\n",
    "from pyDOE import lhs\n",
    "from scipy.interpolate import griddata\n",
    "from smt.sampling_methods import LHS\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from botorch.models.gp_regression import HeteroskedasticSingleTaskGP\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.utils.transforms import normalize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all data\n",
    "data_r1 = pd.read_csv('/Users/ctuwsunlab/Documents/GitHub/PNNL-ML_for_Organic_Flow_Battery_Materials/Round1/extracted_data_round1.csv')\n",
    "data_r2C = pd.read_csv('/Users/ctuwsunlab/Documents/GitHub/PNNL-ML_for_Organic_Flow_Battery_Materials/Round2/ModelC/extracted_data_round2C.csv')\n",
    "data_r3C = pd.read_csv('/Users/ctuwsunlab/Documents/GitHub/PNNL-ML_for_Organic_Flow_Battery_Materials/Round3/ModelC/extracted_data_round3C.csv')\n",
    "data_redo =pd.read_csv('/Users/ctuwsunlab/Documents/GitHub/PNNL-ML_for_Organic_Flow_Battery_Materials/Round_Redo/extracted_data_round_redo.csv')\n",
    "\n",
    "raw_data01 = data_r1[['01_time','01_temp','01_sulf','01_anly','01_yield product']].rename(columns = {'01_time':\"time\",'01_temp':\"temp\",'01_sulf':\"sulf\",'01_anly':\"anly\",'01_yield product':\"yield product\"})\n",
    "raw_data2C = data_r2C[['2C_time','2C_temp','2C_sulf','2C_anly','2C_yield product']].rename(columns = {'2C_time':\"time\",'2C_temp':\"temp\",'2C_sulf':\"sulf\",'2C_anly':\"anly\",'2C_yield product':\"yield product\"})\n",
    "raw_data3C = data_r3C[['3C_time','3C_temp','3C_sulf','3C_anly','3C_yield product']].rename(columns = {'3C_time':\"time\",'3C_temp':\"temp\",'3C_sulf':\"sulf\",'3C_anly':\"anly\",'3C_yield product':\"yield product\"})\n",
    "raw_dataredo = data_redo[['model ID','time','temp','sulf','anly','yield product']]\n",
    "\n",
    "# Deviec set up\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data01 = raw_data01.rolling(window=3).mean().iloc[2::3].reset_index(drop=True)[['time','temp','sulf','anly','yield product']]\n",
    "std_data01 = raw_data01 .rolling(window=3).std().iloc[2::3].reset_index(drop=True)[['yield product']].rename(columns = {'yield product':\"var yield\"})\n",
    "data01 = pd.concat([avg_data01, std_data01], axis=1)\n",
    "\n",
    "avg_data2C = raw_data2C .rolling(window=3).mean().iloc[2::3].reset_index(drop=True)[['time','temp','sulf','anly','yield product']]\n",
    "std_data2C = raw_data2C .rolling(window=3).std().iloc[2::3].reset_index(drop=True)[['yield product']].rename(columns = {'yield product':\"var yield\"})\n",
    "data2C = pd.concat([avg_data2C, std_data2C], axis=1)\n",
    "\n",
    "avg_data3C = raw_data3C.rolling(window=3).mean().iloc[2::3].reset_index(drop=True)[['time','temp','sulf','anly','yield product']]\n",
    "std_data3C = raw_data3C.rolling(window=3).std().iloc[2::3].reset_index(drop=True)[['yield product']].rename(columns = {'yield product':\"var yield\"})\n",
    "data3C = pd.concat([avg_data3C, std_data3C], axis=1)\n",
    "\n",
    "avg_data_redo = raw_dataredo.rolling(window=3).mean().iloc[2::3].reset_index(drop=True)[['model ID','time','temp','sulf','anly','yield product']]\n",
    "std_data_redo = raw_dataredo.rolling(window=3).std().iloc[2::3].reset_index(drop=True)[['yield product']].rename(columns = {'yield product':\"var yield\"})\n",
    "redo_data = pd.concat([avg_data_redo, std_data_redo], axis=1)\n",
    "\n",
    "# Add a category column to each dataframe before concatenating\n",
    "data01['category'] = 'R 01'\n",
    "data2C['category'] = 'R 2C'\n",
    "data3C['category'] = 'R 3C'\n",
    "redo_data['category'] = 'redo'\n",
    "\n",
    "# Concatenate the dataframes\n",
    "modelC_df = pd.concat([data01, data2C, data3C], axis=0, ignore_index=True)\n",
    "outlier_ID = [103, 104] \n",
    "\n",
    "# Replace outliers in modelC_df with their corresponding model IDs in redo_data\n",
    "for outlier_id in outlier_ID:\n",
    "    # Find the corresponding row in redo_data\n",
    "    replacement_row = redo_data[redo_data['model ID'] == outlier_id]\n",
    "    if not replacement_row.empty:\n",
    "        # Replace the row in modelC_df\n",
    "        modelC_df.loc[modelC_df['time'] == replacement_row['time'].values[0], \n",
    "                      ['time', 'temp', 'sulf', 'anly', 'yield product', 'var yield', 'category']] = replacement_row[['time', 'temp', 'sulf', 'anly', 'yield product', 'var yield', 'category']].values\n",
    "\n",
    "\n",
    "# Get high yield samples\n",
    "high_yield_df = modelC_df[modelC_df['yield product'] > 0.9]\n",
    "high_yield_df \n",
    "\n",
    "data3C = modelC_df[(modelC_df['category'] == 'R 3C').idxmax():(modelC_df['category'] == 'R 3C').idxmax()+15].reset_index(drop=True)\n",
    "\n",
    "# Model Sets\n",
    "set_dataC = pd.concat([data01, data2C, data3C], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "bounds = torch.tensor([[30., 20., 75. , 33.], [600., 170., 100.,100.]], dtype=torch.float32) \n",
    "# Define all combinations of variables for triple pair plots\n",
    "variable_combinations = [('time', 'temp', 'sulf'), ('time', 'temp', 'anly'), ('time', 'sulf', 'anly'),('temp', 'sulf', 'anly'),('sulf', 'anly', 'time')]\n",
    "fixed_variable = ['anly', 'sulf', 'temp', 'time']\n",
    "#variable_combinations = [('time', 'temp', 'sulf', 'anly'), ('time', 'sulf', 'anly','temp'), ('temp', 'sulf', 'anly','time'), ('time', 'temp', 'anly','sulf')]\n",
    "\n",
    "def data_prep(df,x1,x2,x3,x4,bounds):\n",
    "    x1 = torch.tensor([df[x1]], dtype=torch.float32).T\n",
    "    x2 = torch.tensor([df[x2]], dtype=torch.float32).T\n",
    "    x3 = torch.tensor([df[x3]], dtype=torch.float32).T\n",
    "    x4 = torch.tensor([df[x4]], dtype=torch.float32).T\n",
    "\n",
    "    train_x = torch.hstack([x1, x2, x3, x4])\n",
    "    train_y = torch.tensor(df['yield product'], dtype=torch.float32).reshape(-1, 1)\n",
    "    train_yvar = torch.tensor(df['var yield'], dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    norm_x = normalize(train_x, bounds)\n",
    "\n",
    "    return norm_x, train_y, train_yvar\n",
    "\n",
    "# Function to generate input data based on variable combination\n",
    "def generate_input_data(A, B, c, d, combination):\n",
    "    if combination == ('time', 'temp', 'sulf'):\n",
    "        return torch.tensor(np.array([[A[i, j], B[i, j], c, d] for i in range(A.shape[0]) for j in range(A.shape[1])]), dtype=dtype)\n",
    "    elif combination == ('time', 'temp', 'anly'):\n",
    "        return torch.tensor(np.array([[A[i, j], B[i, j], d, c] for i in range(A.shape[0]) for j in range(A.shape[1])]), dtype=dtype)\n",
    "    elif combination == ('time', 'sulf', 'anly'):\n",
    "        return torch.tensor(np.array([[A[i, j], d, B[i, j], c ] for i in range(A.shape[0]) for j in range(A.shape[1])]), dtype=dtype)\n",
    "    elif combination == ('temp', 'sulf', 'anly'):\n",
    "        return torch.tensor(np.array([[d, A[i, j], B[i, j], c] for i in range(A.shape[0]) for j in range(A.shape[1])]), dtype=dtype)\n",
    "    elif combination == ('time', 'sulf', 'temp'):\n",
    "        return torch.tensor(np.array([[A[i, j], c,  B[i, j], d] for i in range(A.shape[0]) for j in range(A.shape[1])]), dtype=dtype)\n",
    "    elif combination == ('sulf', 'anly', 'time'):\n",
    "        return torch.tensor(np.array([[c, d,  A[i, j], B[i,j]] for i in range(A.shape[0]) for j in range(A.shape[1])]), dtype=dtype)\n",
    "\n",
    "def gp_4d(train_x, train_y, train_yvar, test_x):\n",
    "    # Define the initial GP model\n",
    "    gp = HeteroskedasticSingleTaskGP(train_X=train_x, train_Y=train_y, train_Yvar=train_yvar) \n",
    "    #gp = SingleTaskGP(train_X=train_x, train_Y=train_y, train_Yvar=train_yvar, covar_module=kernel) \n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    fit_gpytorch_mll(mll)\n",
    "\n",
    "    # Get posterior predictions\n",
    "    gp.eval()\n",
    "    with torch.no_grad():\n",
    "        posterior = gp.posterior(test_x)\n",
    "\n",
    "    mean = posterior.mean.squeeze().numpy()\n",
    "    lower, upper = posterior.mvn.confidence_region()\n",
    "    var = posterior.variance.squeeze().numpy()\n",
    "\n",
    "    return mean, var\n",
    "\n",
    "def create_sclices(norm_x, out_y, noise_y,A,B,c,d_fixed,combination):\n",
    "    mean_values = []\n",
    "    var_values = []\n",
    "    for c in c_slices:\n",
    "        input_data = generate_input_data(A, B, c, d_fixed, combination)\n",
    "        mean, var = gp_4d(norm_x, out_y, noise_y, input_data)\n",
    "        mean_values.append(mean.reshape(A.shape))  # Reshape to match the grid\n",
    "        var_values.append(var.reshape(A.shape))  # Reshape to match the grid\n",
    "\n",
    "    return mean_values, var_values\n",
    "\n",
    "def slcied_plotting(mean_vals1,mean_vals2,mean_vals3,mean_vals4,mean_vals5,combination,minmax,colormap):\n",
    "    global_min = minmax[0]\n",
    "    global_max = minmax[1]\n",
    "    # Create a new figure with subplots for each combination\n",
    "    fig = make_subplots(rows=1, cols=5, subplot_titles=('temp: 0', 'temp: 0.25','temp: 0.5', 'temp: 0.75','temp: 1.0'),\n",
    "                    specs=[[{'type': 'surface'}, {'type': 'surface'},{'type': 'surface'}, {'type': 'surface'}, {'type': 'surface'}]])\n",
    "\n",
    "    for i, (c, y_grid1, y_grid2, y_grid3, y_grid4, y_grid5) in enumerate(zip(c_slices, mean_vals1,mean_vals2,mean_vals3,mean_vals4,mean_vals5), start=1):\n",
    "        fig.add_trace(go.Surface(\n",
    "            x=A,\n",
    "            y=B,\n",
    "            z=c * np.ones_like(A),  # Z-coordinate for slicing\n",
    "            surfacecolor=y_grid1,  # Use predicted `y` as contour\n",
    "            colorscale=colormap,\n",
    "            cmin=global_min,\n",
    "            cmax=global_max,\n",
    "            showscale=True if i == 1 else False,  # Show color scale only on the first slice\n",
    "            #   colorbar_x=0.45,\n",
    "            opacity=0.7\n",
    "        ), row=1, col=1)\n",
    "  \n",
    "        fig.add_trace(go.Surface(\n",
    "            x=A,\n",
    "            y=B,\n",
    "            z=c * np.ones_like(A),  # Z-coordinate for slicing\n",
    "            surfacecolor=y_grid2,  # Use predictediance as contour\n",
    "            cmin=global_min,\n",
    "            cmax=global_max,\n",
    "            colorscale=colormap,\n",
    "            showscale=True if i == 1 else False,  # Show color scale only on the first slice\n",
    "            #colorbar_x=0.45,\n",
    "            opacity=0.7\n",
    "        ), row=1, col=2)\n",
    "\n",
    "        fig.add_trace(go.Surface(\n",
    "            x=A,\n",
    "            y=B,\n",
    "            z=c * np.ones_like(A),  # Z-coordinate for slicing\n",
    "            surfacecolor=y_grid3,  # Use predictediance as contour\n",
    "            cmin=global_min,\n",
    "            cmax=global_max,\n",
    "            colorscale=colormap,\n",
    "            showscale=True if i == 1 else False,  # Show color scale only on the first slice\n",
    "            opacity=0.7\n",
    "        ), row=1, col=3)\n",
    "\n",
    "        fig.add_trace(go.Surface(\n",
    "            x=A,\n",
    "            y=B,\n",
    "            z=c * np.ones_like(A),  # Z-coordinate for slicing\n",
    "            surfacecolor=y_grid4,  # Use predictediance as contour\n",
    "            cmin=global_min,\n",
    "            cmax=global_max,\n",
    "            colorscale=colormap,\n",
    "            showscale=True if i == 1 else False,  # Show color scale only on the first slice\n",
    "            opacity=0.7\n",
    "        ), row=1, col=4)\n",
    "\n",
    "        fig.add_trace(go.Surface(\n",
    "            x=A,\n",
    "            y=B,\n",
    "            z=c * np.ones_like(A),  # Z-coordinate for slicing\n",
    "            surfacecolor=y_grid5,  # Use predictediance as contour\n",
    "            cmin=global_min,\n",
    "            cmax=global_max,\n",
    "            colorscale=colormap,\n",
    "            showscale=True if i == 1 else False,  # Show color scale only on the first slice\n",
    "            opacity=0.7\n",
    "        ), row=1, col=5)\n",
    "        \n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        width=1300,\n",
    "        margin=dict(l=50, r=50, b=50, t=50),\n",
    "        scene=dict(\n",
    "            xaxis_title=combination[0],\n",
    "            yaxis_title=combination[1],\n",
    "            zaxis_title=combination[2]\n",
    "        ),\n",
    "        scene2=dict(\n",
    "            xaxis_title=combination[0],\n",
    "            yaxis_title=combination[1],\n",
    "            zaxis_title=combination[2]\n",
    "        ),\n",
    "        scene3=dict(\n",
    "            xaxis_title=combination[0],\n",
    "            yaxis_title=combination[1],\n",
    "            zaxis_title=combination[2]\n",
    "        ),\n",
    "        scene4=dict(\n",
    "            xaxis_title=combination[0],\n",
    "            yaxis_title=combination[1],\n",
    "            zaxis_title=combination[2]\n",
    "        ),\n",
    "        scene5=dict(\n",
    "            xaxis_title=combination[0],\n",
    "            yaxis_title=combination[1],\n",
    "            zaxis_title=combination[2]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "comb_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid for demonstration (adjust as needed for your data range)\n",
    "num_points = 15  # Points per axis for plotting\n",
    "a = np.linspace(0, 1, num_points)\n",
    "b = np.linspace(0, 1, num_points)\n",
    "c_slices = np.linspace(0, 1, 12)  # Slicing variable\n",
    "d_fixed =  0.5 # Example fixed value for d - three temperatures\n",
    "\n",
    "# # Create meshgrid for a, b\n",
    "A, B = np.meshgrid(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "norm_0, out_0, noise_0 = data_prep(data00,'time','temp','sulf','anly',bounds)\n",
    "\n",
    "mean_vals1, var_vals1 = create_sclices(norm_0, out_0, noise_0, A,B,c_slices,0.0 ,variable_combinations[comb_id]) # fixed variable: temp = 0.0\n",
    "mean_vals2, var_vals2 = create_sclices(norm_0, out_0, noise_0, A,B,c_slices,0.25,variable_combinations[comb_id]) # fixed variable: temp = 0.25\n",
    "mean_vals3, var_vals3 = create_sclices(norm_0, out_0, noise_0, A,B,c_slices,0.5 ,variable_combinations[comb_id]) # fixed variable: temp = 0.50\n",
    "mean_vals4, var_vals4 = create_sclices(norm_0, out_0, noise_0, A,B,c_slices,0.75,variable_combinations[comb_id]) # fixed variable: temp = 0.75\n",
    "mean_vals5, var_vals5 = create_sclices(norm_0, out_0, noise_0, A,B,c_slices,1.0 ,variable_combinations[comb_id]) # fixed variable: temp = 1.0\n",
    "\n",
    "\n",
    "slcied_plotting(mean_vals1,mean_vals2,mean_vals3,mean_vals4,mean_vals5,variable_combinations[comb_id],[0,1],'Cividis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data2C = pd.concat([data01, data2C ], ignore_index=True)\n",
    "norm_2C, out_2C, noise_2C = data_prep(set_data2C,'time','temp','sulf','anly',bounds)\n",
    "\n",
    "mean_vals1, var_vals1 = create_sclices(norm_2C, out_2C, noise_2C, A,B,c_slices,0.0 ,variable_combinations[comb_id]) # fixed variable: temp = 0.00\n",
    "mean_vals2, var_vals2 = create_sclices(norm_2C, out_2C, noise_2C, A,B,c_slices,0.25,variable_combinations[comb_id]) # fixed variable: temp = 0.25\n",
    "mean_vals3, var_vals3 = create_sclices(norm_2C, out_2C, noise_2C, A,B,c_slices,0.5 ,variable_combinations[comb_id]) # fixed variable: temp = 0.50\n",
    "mean_vals4, var_vals4 = create_sclices(norm_2C, out_2C, noise_2C, A,B,c_slices,0.75,variable_combinations[comb_id]) # fixed variable: temp = 0.75\n",
    "mean_vals5, var_vals5 = create_sclices(norm_2C, out_2C, noise_2C, A,B,c_slices,1.0 ,variable_combinations[comb_id]) # fixed variable: temp = 1.00\n",
    "\n",
    "slcied_plotting(mean_vals1,mean_vals2,mean_vals3,mean_vals4,mean_vals5,variable_combinations[comb_id],[0,1],'Cividis')\n",
    "\n",
    "abs_min = min([np.min(var_vals1),np.min(var_vals2),np.min(var_vals3),np.min(var_vals4),np.min(var_vals5)])\n",
    "abs_max = max([np.max(var_vals1),np.max(var_vals2),np.max(var_vals3),np.max(var_vals4),np.max(var_vals5)])\n",
    "slcied_plotting(var_vals1,var_vals2,var_vals3,var_vals4,var_vals5,variable_combinations[comb_id],[abs_min,abs_max],'Cividis_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data3C = pd.concat([data01, data2A, data3C], axis=0, ignore_index=True)\n",
    "norm_3C, out_3C, noise_3C = data_prep(set_data3C,'time','temp','sulf','anly',bounds)\n",
    "mean_vals1, var_vals1 = create_sclices(norm_3C, out_3C, noise_3C, A,B,c_slices,0.0 ,variable_combinations[comb_id]) # fixed variable: temp = 0.0\n",
    "mean_vals2, var_vals2 = create_sclices(norm_3C, out_3C, noise_3C, A,B,c_slices,0.25,variable_combinations[comb_id]) # fixed variable: temp = 0.25\n",
    "mean_vals3, var_vals3 = create_sclices(norm_3C, out_3C, noise_3C, A,B,c_slices,0.5 ,variable_combinations[comb_id]) # fixed variable: temp = 0.50\n",
    "mean_vals4, var_vals4 = create_sclices(norm_3C, out_3C, noise_3C, A,B,c_slices,0.75,variable_combinations[comb_id]) # fixed variable: temp = 0.75\n",
    "mean_vals5, var_vals5 = create_sclices(norm_3C, out_3C, noise_3C, A,B,c_slices,1.0 ,variable_combinations[comb_id]) # fixed variable: temp = 1.0\n",
    "\n",
    "slcied_plotting(mean_vals1,mean_vals2,mean_vals3,mean_vals4,mean_vals5,variable_combinations[comb_id],[0,1],'Cividis')\n",
    "\n",
    "abs_min = min([np.min(var_vals1),np.min(var_vals2),np.min(var_vals3),np.min(var_vals4),np.min(var_vals5)])\n",
    "abs_max = max([np.max(var_vals1),np.max(var_vals2),np.max(var_vals3),np.max(var_vals4),np.max(var_vals5)])\n",
    "slcied_plotting(var_vals1,var_vals2,var_vals3,var_vals4,var_vals5,variable_combinations[comb_id],[abs_min,abs_max],'Cividis_r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
